This homework was a stiff learning curve for me since we needed to deal with new kind of MapReduce tools. Both Hive and Spark was at first a little challenging for me at first because i was so used to the Hadoop's system of job registration through Java. However, as I got more used to the syntax of Scala, I was more enabled in easily registering multiple MapReduce jobs without having to code out each components in Java, and that was a huge time saver. I was surprised as I was going through the problem sets that such complex jobs were instantly called via a single line of code through Spark. I do, however, think that I need to get used to the Sparks lack of output of logs while on the job, since I'm so used to observing the jobs go and being ensured that it's going well.
