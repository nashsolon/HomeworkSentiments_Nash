From this homework,I have learned a lot of things. The most important one is the concept of skew and the way to reduce skew. For my understanding, the generation of skew in hadoop results from the unbalanced data. To be specifically, the occurence of some keys are very large while the occurence of some keys are very some.Under this condition, the reduce tasks for different keys have different run time, which leads to skew. Of course, we have methods to solve this problem, the first solution is to let the number of Reduce tasks be fewer than the number of reduce functions. The basic idea of this soluion is to average the runtime of each Reduce task; The other solution is to let the number of Reduce tasks be larger than the number of compute nodes. The basic idea of this solution is to average the running time of each compute node. To be honest, I dislike the third problem. Actually, I am confused about what this problem whant us do. Although I have read the books, I still couldn't have a better understanding about How YARN works. I know YARN is hadoop's resource management system and it frees the original Jobtracker from the heavy load. But how Job execution and Job submission on YARN is still a mystery for me. I do hope further study can help me get a better and deeper understanding.
